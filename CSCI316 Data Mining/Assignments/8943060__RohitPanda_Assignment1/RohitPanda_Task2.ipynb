{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Assignment 1 Task 2\n",
        "# Name: Rohit Panda\n",
        "# UOW ID: 8943060"
      ],
      "metadata": {
        "id": "_iAYE7CdDftV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before Running the code, ensure you have the kaggle.json file to upload into the Colab.\n",
        "\n",
        "Download the kaggle.json from this link:\n",
        "[Download Now](https://drive.google.com/file/d/15J5mknfkekOMMzW0bw6tjVwEmJj-e_C_/view?usp=sharing)"
      ],
      "metadata": {
        "id": "sEbJqwr8fCS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading & Setup\n",
        "We load the Drug Classification dataset from Kaggle using the Kaggle API. The dataset is processed using pandas for further analysis and preparation. The dataset contains categorical and continuous attributes to predict drug type.\n"
      ],
      "metadata": {
        "id": "VrEI7UESOaO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Gqd6hepgDefV",
        "outputId": "317e05b8-e0d3-40ff-9c7e-a9519f11fe88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-82d259aa-69c2-4435-9f07-8d4ab647b6fc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-82d259aa-69c2-4435-9f07-8d4ab647b6fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# ========== Task 2: Setup and Dataset Download ==========\n",
        "\n",
        "# Step 0: Upload kaggle.json\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload kaggle.json manually here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Move kaggle.json to the correct location\n",
        "import os\n",
        "if not os.path.exists(os.path.expanduser(\"~/.kaggle\")):\n",
        "    os.makedirs(os.path.expanduser(\"~/.kaggle\"))\n",
        "os.rename(\"kaggle.json\", os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
        "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)"
      ],
      "metadata": {
        "id": "6mjlolbiK38B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install Kaggle API (if not already installed)\n",
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "58OkAs_KK_sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Download new dataset (replace this with your specific dataset name)\n",
        "dataset_name = \"prathamtripathi/drug-classification\"\n",
        "!kaggle datasets download -d {dataset_name}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PKFI5wtLC0-",
        "outputId": "82f780fa-3123-4c35-90d1-de078e861fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/prathamtripathi/drug-classification\n",
            "License(s): CC0-1.0\n",
            "Downloading drug-classification.zip to /content\n",
            "  0% 0.00/1.68k [00:00<?, ?B/s]\n",
            "100% 1.68k/1.68k [00:00<00:00, 6.61MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Unzip dataset\n",
        "import zipfile\n",
        "zip_file = dataset_name.split(\"/\")[-1] + \".zip\"  # credit-score-classification.zip\n",
        "extract_path = \"task2_data\"\n",
        "\n",
        "if os.path.exists(zip_file):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"✅ Dataset extracted to {extract_path}/\")\n",
        "else:\n",
        "    print(f\"❌ ERROR: Zip file {zip_file} not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIh3YB-dLQDl",
        "outputId": "cea55158-8624-43f9-8f35-b5672af78c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset extracted to task2_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load CSV (adjust the filename if different)\n",
        "import pandas as pd\n",
        "csv_path = f\"{extract_path}/drug200.csv\"  # Replace with test.csv or other if needed\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"✅ DataFrame loaded successfully!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"First 5 rows:\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f\"❌ ERROR: CSV file not found at {csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP6lR0CULQhE",
        "outputId": "65a74fd3-52eb-40c3-cf85-8172c99588ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DataFrame loaded successfully!\n",
            "Shape: (200, 6)\n",
            "First 5 rows:\n",
            "   Age Sex      BP Cholesterol  Na_to_K   Drug\n",
            "0   23   F    HIGH        HIGH   25.355  DrugY\n",
            "1   47   M     LOW        HIGH   13.093  drugC\n",
            "2   47   M     LOW        HIGH   10.114  drugC\n",
            "3   28   F  NORMAL        HIGH    7.798  drugX\n",
            "4   61   F     LOW        HIGH   18.043  DrugY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "Ir0eafLLLriF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing and Binning\n",
        "All missing values are handled using mean (for numeric) and mode (for categorical) imputation. Then, all continuous attributes are transformed into categorical bins (`Low`, `Medium`, `High`) using quantile-based binning (`pd.qcut()`).\n"
      ],
      "metadata": {
        "id": "xGraOa-jOd7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Task 2: Drug Classification with Decision Tree ===\")\n",
        "\n",
        "# Load the dataset\n",
        "print(\"\\n1. Loading Dataset\")\n",
        "# Note: Replace 'drug200.csv' with your actual file path\n",
        "df = pd.read_csv(f\"{extract_path}/drug200.csv\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Data preprocessing\n",
        "print(\"\\n2. Data Preprocessing\")\n",
        "\n",
        "# Check data types and unique values\n",
        "print(\"Data types and unique values:\")\n",
        "for col in df.columns:\n",
        "    print(f\"{col}: {df[col].dtype}, unique values: {df[col].nunique()}\")\n",
        "    if df[col].dtype == 'object':\n",
        "        print(f\"  Values: {df[col].unique()}\")\n",
        "\n",
        "# Handle missing values if any\n",
        "def preprocess_data(data):\n",
        "    \"\"\"Preprocess the data by handling missing values\"\"\"\n",
        "    processed_data = data.copy()\n",
        "\n",
        "    for col in processed_data.columns:\n",
        "        if processed_data[col].isnull().sum() > 0:\n",
        "            if processed_data[col].dtype == 'object':\n",
        "                # Fill categorical with mode\n",
        "                mode_val = processed_data[col].mode()[0]\n",
        "                processed_data[col] = processed_data[col].fillna(mode_val)\n",
        "                print(f\"Filled {col} missing values with mode: {mode_val}\")\n",
        "            else:\n",
        "                # Fill numerical with mean\n",
        "                mean_val = processed_data[col].mean()\n",
        "                processed_data[col] = processed_data[col].fillna(mean_val)\n",
        "                print(f\"Filled {col} missing values with mean: {mean_val:.2f}\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "df = preprocess_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vs_81TsLjWf",
        "outputId": "a2334d1d-e6fd-4e19-c648-ebd76a60144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Task 2: Drug Classification with Decision Tree ===\n",
            "\n",
            "1. Loading Dataset\n",
            "Dataset shape: (200, 6)\n",
            "Columns: ['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug']\n",
            "\n",
            "First 5 rows:\n",
            "   Age Sex      BP Cholesterol  Na_to_K   Drug\n",
            "0   23   F    HIGH        HIGH   25.355  DrugY\n",
            "1   47   M     LOW        HIGH   13.093  drugC\n",
            "2   47   M     LOW        HIGH   10.114  drugC\n",
            "3   28   F  NORMAL        HIGH    7.798  drugX\n",
            "4   61   F     LOW        HIGH   18.043  DrugY\n",
            "\n",
            "Missing values per column:\n",
            "Age            0\n",
            "Sex            0\n",
            "BP             0\n",
            "Cholesterol    0\n",
            "Na_to_K        0\n",
            "Drug           0\n",
            "dtype: int64\n",
            "\n",
            "2. Data Preprocessing\n",
            "Data types and unique values:\n",
            "Age: int64, unique values: 57\n",
            "Sex: object, unique values: 2\n",
            "  Values: ['F' 'M']\n",
            "BP: object, unique values: 3\n",
            "  Values: ['HIGH' 'LOW' 'NORMAL']\n",
            "Cholesterol: object, unique values: 2\n",
            "  Values: ['HIGH' 'NORMAL']\n",
            "Na_to_K: float64, unique values: 198\n",
            "Drug: object, unique values: 5\n",
            "  Values: ['DrugY' 'drugC' 'drugX' 'drugA' 'drugB']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) Use binning to transform continuous attributes into discrete values\n",
        "print(\"\\n3. Binning Continuous Attributes\")\n",
        "\n",
        "continuous_cols = df.select_dtypes(include=[np.number]).columns\n",
        "continuous_cols = [col for col in continuous_cols if col != 'Drug']  # Exclude target if it's numeric\n",
        "\n",
        "print(f\"Continuous columns found: {list(continuous_cols)}\")\n",
        "\n",
        "for col in continuous_cols:\n",
        "    # Create 3 bins for each continuous attribute\n",
        "    df[f'{col}_binned'] = pd.qcut(df[col], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
        "    print(f\"Binned {col} into 3 categories\")\n",
        "    print(f\"  Bin distribution: {df[f'{col}_binned'].value_counts().to_dict()}\")\n",
        "\n",
        "# Encode categorical variables\n",
        "print(\"\\n4. Encoding Categorical Variables\")\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "categorical_cols = [col for col in categorical_cols if col != 'Drug']  # Exclude target\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df[col + '_encoded'] = pd.Categorical(df[col]).codes\n",
        "    print(f\"Encoded {col}: {dict(enumerate(df[col].unique()))}\")\n",
        "\n",
        "# Prepare final dataset for decision tree\n",
        "# Select encoded/binned features and target\n",
        "feature_cols = []\n",
        "for col in df.columns:\n",
        "    if col.endswith('_binned') or col.endswith('_encoded'):\n",
        "        feature_cols.append(col)\n",
        "\n",
        "# If no binned columns, use original categorical columns encoded\n",
        "if not feature_cols:\n",
        "    # Encode all categorical columns\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        if col != 'Drug':\n",
        "            df[col + '_encoded'] = pd.Categorical(df[col]).codes\n",
        "            feature_cols.append(col + '_encoded')\n",
        "\n",
        "    # Include original continuous columns\n",
        "    for col in continuous_cols:\n",
        "        feature_cols.append(col)\n",
        "\n",
        "# Add target column\n",
        "target_col = 'Drug'\n",
        "if df[target_col].dtype == 'object':\n",
        "    df[target_col + '_encoded'] = pd.Categorical(df[target_col]).codes\n",
        "    target_col = target_col + '_encoded'\n",
        "\n",
        "print(f\"Selected features: {feature_cols}\")\n",
        "print(f\"Target column: {target_col}\")\n",
        "\n",
        "# Create final dataset\n",
        "final_df = df[feature_cols + [target_col]].copy()\n",
        "print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
        "print(\"Final dataset head:\")\n",
        "print(final_df.head())\n",
        "\n",
        "# (2) Split data into 80% training and 20% test\n",
        "print(\"\\n5. Splitting Data (80% Train, 20% Test)\")\n",
        "\n",
        "def train_test_split(data, test_size=0.2, random_state=42):\n",
        "    \"\"\"Split data into training and testing sets\"\"\"\n",
        "    random.seed(random_state)\n",
        "    n_test = int(len(data) * test_size)\n",
        "\n",
        "    # Randomly select test indices\n",
        "    test_indices = random.sample(range(len(data)), n_test)\n",
        "    train_indices = [i for i in range(len(data)) if i not in test_indices]\n",
        "\n",
        "    train_data = data.iloc[train_indices].reset_index(drop=True)\n",
        "    test_data = data.iloc[test_indices].reset_index(drop=True)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "train_data, test_data = train_test_split(final_df)\n",
        "\n",
        "print(f\"Training set size: {len(train_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wb-Nt-ALyoQ",
        "outputId": "902dad33-b594-48f1-e76f-efa39f161ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Binning Continuous Attributes\n",
            "Continuous columns found: ['Age', 'Na_to_K']\n",
            "Binned Age into 3 categories\n",
            "  Bin distribution: {'Low': 70, 'High': 67, 'Medium': 63}\n",
            "Binned Na_to_K into 3 categories\n",
            "  Bin distribution: {'Low': 67, 'High': 67, 'Medium': 66}\n",
            "\n",
            "4. Encoding Categorical Variables\n",
            "Encoded Sex: {0: 'F', 1: 'M'}\n",
            "Encoded BP: {0: 'HIGH', 1: 'LOW', 2: 'NORMAL'}\n",
            "Encoded Cholesterol: {0: 'HIGH', 1: 'NORMAL'}\n",
            "Selected features: ['Age_binned', 'Na_to_K_binned', 'Sex_encoded', 'BP_encoded', 'Cholesterol_encoded']\n",
            "Target column: Drug_encoded\n",
            "\n",
            "Final dataset shape: (200, 6)\n",
            "Final dataset head:\n",
            "  Age_binned Na_to_K_binned  Sex_encoded  BP_encoded  Cholesterol_encoded  \\\n",
            "0        Low           High            0           0                    0   \n",
            "1     Medium         Medium            1           1                    0   \n",
            "2     Medium            Low            1           1                    0   \n",
            "3        Low            Low            0           2                    0   \n",
            "4       High           High            0           1                    0   \n",
            "\n",
            "   Drug_encoded  \n",
            "0             0  \n",
            "1             3  \n",
            "2             3  \n",
            "3             4  \n",
            "4             0  \n",
            "\n",
            "5. Splitting Data (80% Train, 20% Test)\n",
            "Training set size: 160\n",
            "Test set size: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classifier\n",
        "We implement a decision tree classifier from scratch using information gain and entropy. The tree is built recursively with optional pre-pruning via `max_depth` and `min_samples_split` to avoid overfitting.\n"
      ],
      "metadata": {
        "id": "1FIMRRUzOn7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (3) Decision Tree Implementation\n",
        "print(\"\\n6. Decision Tree Implementation\")\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "        self.feature_names = None\n",
        "        self.target_name = None\n",
        "\n",
        "    def calculate_entropy(self, y):\n",
        "        \"\"\"Calculate entropy of a target variable\"\"\"\n",
        "        if len(y) == 0:\n",
        "            return 0\n",
        "\n",
        "        _, counts = np.unique(y, return_counts=True)\n",
        "        probabilities = counts / len(y)\n",
        "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "    def calculate_information_gain(self, X, y, feature_idx, threshold=None):\n",
        "        \"\"\"Calculate information gain for a feature\"\"\"\n",
        "        parent_entropy = self.calculate_entropy(y)\n",
        "\n",
        "        # For categorical features, split by unique values\n",
        "        unique_values = np.unique(X[:, feature_idx])\n",
        "        if len(unique_values) <= 1:\n",
        "            return 0, None # Return 0 gain and None threshold\n",
        "\n",
        "        best_gain = 0\n",
        "        best_threshold = None\n",
        "\n",
        "        for value in unique_values:\n",
        "            left_mask = X[:, feature_idx] == value\n",
        "            right_mask = ~left_mask\n",
        "\n",
        "            if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
        "                continue\n",
        "\n",
        "            left_entropy = self.calculate_entropy(y[left_mask])\n",
        "            right_entropy = self.calculate_entropy(y[right_mask])\n",
        "\n",
        "            weighted_entropy = (np.sum(left_mask) / len(y)) * left_entropy + \\\n",
        "                             (np.sum(right_mask) / len(y)) * right_entropy\n",
        "\n",
        "            gain = parent_entropy - weighted_entropy\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_threshold = value\n",
        "\n",
        "        return best_gain, best_threshold\n",
        "\n",
        "\n",
        "    def find_best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split on\"\"\"\n",
        "        best_gain = 0\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            gain, threshold = self.calculate_information_gain(X, y, feature_idx)\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def build_tree(self, X, y, depth=0):\n",
        "        \"\"\"Recursively build the decision tree\"\"\"\n",
        "        # Base cases\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return {'class': y[0], 'samples': len(y)}\n",
        "\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
        "           len(y) < self.min_samples_split:\n",
        "            # Return most common class\n",
        "            unique_classes, counts = np.unique(y, return_counts=True)\n",
        "            majority_class = unique_classes[np.argmax(counts)]\n",
        "            return {'class': majority_class, 'samples': len(y)}\n",
        "\n",
        "        # Find best split\n",
        "        best_feature, best_threshold, best_gain = self.find_best_split(X, y)\n",
        "\n",
        "        if best_feature is None or best_gain == 0:\n",
        "            # No good split found\n",
        "            unique_classes, counts = np.unique(y, return_counts=True)\n",
        "            majority_class = unique_classes[np.argmax(counts)]\n",
        "            return {'class': majority_class, 'samples': len(y)}\n",
        "\n",
        "        # Split data\n",
        "        left_mask = X[:, best_feature] == best_threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        # Recursively build subtrees\n",
        "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_feature,\n",
        "            'threshold': best_threshold,\n",
        "            'left': left_tree,\n",
        "            'right': right_tree,\n",
        "            'samples': len(y)\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the decision tree\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.feature_names = X.columns.tolist()\n",
        "            X = X.values\n",
        "\n",
        "        if isinstance(y, pd.Series):\n",
        "            self.target_name = y.name\n",
        "            y = y.values\n",
        "\n",
        "        self.tree = self.build_tree(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_single(self, x, tree):\n",
        "        \"\"\"Predict a single sample\"\"\"\n",
        "        if 'class' in tree:\n",
        "            return tree['class']\n",
        "\n",
        "        if x[tree['feature']] == tree['threshold']:\n",
        "            return self.predict_single(x, tree['left'])\n",
        "        else:\n",
        "            return self.predict_single(x, tree['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict multiple samples\"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            predictions.append(self.predict_single(x, self.tree))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def print_tree(self, tree=None, depth=0):\n",
        "        \"\"\"Print the decision tree structure\"\"\"\n",
        "        if tree is None:\n",
        "            tree = self.tree\n",
        "\n",
        "        indent = \"  \" * depth\n",
        "\n",
        "        if 'class' in tree:\n",
        "            print(f\"{indent}Predict: {tree['class']} (samples: {tree['samples']})\")\n",
        "        else:\n",
        "            feature_name = self.feature_names[tree['feature']] if self.feature_names else f\"Feature_{tree['feature']}\"\n",
        "            print(f\"{indent}If {feature_name} == {tree['threshold']}:\")\n",
        "            self.print_tree(tree['left'], depth + 1)\n",
        "            print(f\"{indent}Else:\")\n",
        "            self.print_tree(tree['right'], depth + 1)\n",
        "\n",
        "# Train the decision tree\n",
        "print(\"Training Decision Tree...\")\n",
        "\n",
        "# Prepare training data\n",
        "X_train = train_data[feature_cols]\n",
        "y_train = train_data[target_col]\n",
        "\n",
        "# Initialize and train the decision tree\n",
        "dt = DecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Decision Tree trained successfully!\")\n",
        "\n",
        "# Print tree structure\n",
        "print(\"\\nDecision Tree Structure:\")\n",
        "dt.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmRzRGNsL-W2",
        "outputId": "a7bb9db9-f514-44f4-cdd6-ab3711e75c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6. Decision Tree Implementation\n",
            "Training Decision Tree...\n",
            "Decision Tree trained successfully!\n",
            "\n",
            "Decision Tree Structure:\n",
            "If BP_encoded == 0:\n",
            "  If Na_to_K_binned == High:\n",
            "    Predict: 0 (samples: 23)\n",
            "  Else:\n",
            "    If Age_binned == High:\n",
            "      If Na_to_K_binned == Low:\n",
            "        Predict: 2 (samples: 6)\n",
            "      Else:\n",
            "        If Sex_encoded == 0:\n",
            "          Predict: 2 (samples: 3)\n",
            "        Else:\n",
            "          Predict: 2 (samples: 3)\n",
            "    Else:\n",
            "      If Na_to_K_binned == Low:\n",
            "        If Age_binned == Low:\n",
            "          Predict: 1 (samples: 6)\n",
            "        Else:\n",
            "          Predict: 1 (samples: 7)\n",
            "      Else:\n",
            "        If Cholesterol_encoded == 0:\n",
            "          Predict: 1 (samples: 6)\n",
            "        Else:\n",
            "          Predict: 0 (samples: 9)\n",
            "Else:\n",
            "  If Na_to_K_binned == High:\n",
            "    Predict: 0 (samples: 27)\n",
            "  Else:\n",
            "    If Cholesterol_encoded == 0:\n",
            "      If BP_encoded == 1:\n",
            "        If Na_to_K_binned == Low:\n",
            "          Predict: 3 (samples: 9)\n",
            "        Else:\n",
            "          Predict: 3 (samples: 7)\n",
            "      Else:\n",
            "        If Na_to_K_binned == Low:\n",
            "          Predict: 4 (samples: 8)\n",
            "        Else:\n",
            "          Predict: 4 (samples: 11)\n",
            "    Else:\n",
            "      If Age_binned == Medium:\n",
            "        If Na_to_K_binned == Low:\n",
            "          Predict: 4 (samples: 8)\n",
            "        Else:\n",
            "          Predict: 4 (samples: 8)\n",
            "      Else:\n",
            "        Predict: 4 (samples: 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and Accuracy Evaluation\n",
        "We test the decision tree on the test set and evaluate its performance using accuracy. Additionally, we print predicted vs actual results and class distribution to analyze misclassifications.\n"
      ],
      "metadata": {
        "id": "oETOSQuMOrEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (4) Test the classifier\n",
        "print(\"\\n7. Testing the Classifier\")\n",
        "\n",
        "# Prepare test data\n",
        "X_test = test_data[feature_cols]\n",
        "y_test = test_data[target_col]\n",
        "\n",
        "# Make predictions\n",
        "predictions = dt.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predictions == y_test.values)\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Display classification results\n",
        "print(\"\\nClassification Results (first 10 samples):\")\n",
        "print(\"Predicted | Actual\")\n",
        "print(\"-\" * 18)\n",
        "for i in range(min(10, len(predictions))):\n",
        "    print(f\"{predictions[i]:>9} | {y_test.iloc[i]:>6}\")\n",
        "\n",
        "# Show confusion matrix-like statistics\n",
        "unique_classes = np.unique(np.concatenate([predictions, y_test.values]))\n",
        "print(f\"\\nClass distribution in test set:\")\n",
        "for cls in unique_classes:\n",
        "    actual_count = np.sum(y_test.values == cls)\n",
        "    predicted_count = np.sum(predictions == cls)\n",
        "    print(f\"Class {cls}: Actual={actual_count}, Predicted={predicted_count}\")\n",
        "\n",
        "print(f\"\\nTotal test samples: {len(y_test)}\")\n",
        "print(f\"Correctly classified: {np.sum(predictions == y_test.values)}\")\n",
        "print(f\"Incorrectly classified: {np.sum(predictions != y_test.values)}\")\n",
        "\n",
        "print(\"\\n=== Task 2 Completed Successfully ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah7RQfBSMD0n",
        "outputId": "185dbaf1-d21d-4d36-f92e-de2a562300d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7. Testing the Classifier\n",
            "Test Accuracy: 0.8500 (85.00%)\n",
            "\n",
            "Classification Results (first 10 samples):\n",
            "Predicted | Actual\n",
            "------------------\n",
            "        0 |      0\n",
            "        0 |      0\n",
            "        4 |      0\n",
            "        0 |      0\n",
            "        2 |      2\n",
            "        0 |      0\n",
            "        0 |      0\n",
            "        4 |      4\n",
            "        0 |      0\n",
            "        0 |      0\n",
            "\n",
            "Class distribution in test set:\n",
            "Class 0: Actual=23, Predicted=17\n",
            "Class 1: Actual=2, Predicted=2\n",
            "Class 2: Actual=4, Predicted=5\n",
            "Class 3: Actual=3, Predicted=5\n",
            "Class 4: Actual=8, Predicted=11\n",
            "\n",
            "Total test samples: 40\n",
            "Correctly classified: 34\n",
            "Incorrectly classified: 6\n",
            "\n",
            "=== Task 2 Completed Successfully ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#END OF ASSIGNMENT 1 TASK 2"
      ],
      "metadata": {
        "id": "aLFnljmrMGz2"
      }
    }
  ]
}
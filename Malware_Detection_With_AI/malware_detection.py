# Import necessary libraries
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib  # For saving the model

# File path
file_path = "./data/top_1000_pe_imports.xlsx"

# Check if file exists
if not os.path.exists(file_path):
    raise FileNotFoundError(f"The file at {file_path} does not exist!")

# Load the dataset
def load_dataset(file_path, nrows=None):
    try:
        print(f"Loading dataset from {file_path}...")
        data = pd.read_excel(file_path, engine="openpyxl", nrows=nrows)
        print("Dataset loaded successfully!")
        print(f"First 5 rows of the dataset:\n{data.head()}")
        return data
    except Exception as e:
        raise ValueError(f"Error loading dataset: {e}")

def preprocess_data(data):
    print("\nPreprocessing dataset...")
    
    # Check if the label column exists and dynamically adjust
    label_column = "malware"  # Update this based on your dataset
    if label_column not in data.columns:
        raise ValueError(f"The dataset does not contain a '{label_column}' column.")
    print(f"Number of features in dataset: {data.shape[1]}")  # Total columns, including the label
    print(data.columns)  # List all column names

    # Separate features and labels
    X = data.drop(columns=[label_column, "hash"], axis=1)  # Drop label and irrelevant columns like 'hash'
    y = data[label_column]
    
    # Normalize the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print("Preprocessing complete!")
    return X_scaled, y, scaler


# Train a model
def train_model(X_train, y_train):
    print("\nTraining the model...")
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    print("Model training complete!")
    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    print("\nEvaluating the model...")
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy * 100:.2f}%")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    return y_pred, cm

# Visualize the confusion matrix
def visualize_confusion_matrix(cm):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malicious'], yticklabels=['Benign', 'Malicious'])
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

# Save the model and scaler
def save_model(model, scaler, model_path="model.pkl", scaler_path="scaler.pkl"):
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    print(f"Model saved to {model_path}")
    print(f"Scaler saved to {scaler_path}")


# Main function
if __name__ == "__main__":
    try:
        # Step 1: Load the dataset
        data = load_dataset(file_path)
        
        # Step 2: Preprocess the data
        X, y, scaler = preprocess_data(data)
        
        # Step 3: Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Step 4: Train the model
        model = train_model(X_train, y_train)
        
        # Step 5: Evaluate the model
        y_pred, cm = evaluate_model(model, X_test, y_test)
        
        # Step 6: Visualize the results
        visualize_confusion_matrix(cm)
        
        # Step 7: Save the model and scaler for later use
        save_model(model, scaler)
        
    except Exception as e:
        print(f"An error occurred: {e}")


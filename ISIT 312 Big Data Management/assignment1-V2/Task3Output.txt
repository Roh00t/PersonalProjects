bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task2$ cd ../ && cd task3
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ cd ~/Desktop/assignment1/task3
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ ls -lh sales.txt
-rw-rw-rw- 1 bigdata bigdata 286 Oct  1 21:52 sales.txt
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ head sales.txt
bolt 45
washer 7
bolt 51
washer 10
screw 48
screw 13
nail 50
washer 3
washer 56
screw 28
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -mkdir -p /assign1/task3/in
25/10/01 22:30:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -put -f sales.txt /assign1/task3/in/
25/10/01 22:30:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -ls /assign1/task3/in
25/10/01 22:30:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   1 bigdata supergroup        286 2025-10-01 22:30 /assign1/task3/in/sales.txt
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -cat /assign1/task3/in/sales.txt | head
25/10/01 22:30:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
bolt 45
washer 7
bolt 51
washer 10
screw 48
screw 13
nail 50
washer 3
washer 56
screw 28
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ gedit solution3.java

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4b90 is not mapped

(gedit:8839): Gtk-WARNING **: GtkScrolledWindow 0x10ea880 is mapped but visible child GtkScrollbar 0x10f4d90 is not mapped
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ cat solution3.java 
// Student Name: Rohit Panda
// Student UOW ID: 8943060

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


public class solution3 {

    // Mapper Class
    public static class SalesMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        private Text item = new Text();
        private IntWritable amount = new IntWritable();

        @Override
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split("\\s+");
            if (parts.length == 2) {
                try {
                    item.set(parts[0]);
                    amount.set(Integer.parseInt(parts[1]));
                    context.write(item, amount);
                } catch (NumberFormatException e) {
                    // skip malformed lines
                }
            }
        }
    }

    // Reducer Class
    public static class StatsReducer extends Reducer<Text, IntWritable, Text, Text> {
        @Override
        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int max = Integer.MIN_VALUE;
            int min = Integer.MAX_VALUE;
            int sum = 0;
            int count = 0;

            for (IntWritable val : values) {
                int v = val.get();
                if (v > max) max = v;
                if (v < min) min = v;
                sum += v;
                count++;
            }

            double avg = (count == 0) ? 0.0 : (double) sum / count;
            String result = String.format("MAX=%d MIN=%d AVG=%.2f SUM=%d", max, min, avg, sum);

            context.write(key, new Text(result));
        }
    }

    // Driver Method
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: solution3 <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Sales Statistics");

        job.setJarByClass(solution3.class);
        job.setMapperClass(SalesMapper.class);
        job.setReducerClass(StatsReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ rm -rf classes && mkdir classes
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ javac -classpath "$(hadoop classpath)" -d classes solution3.java
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ jar -cvf solution3.jar -C classes .
added manifest
adding: solution3.class(in = 1701) (out= 929)(deflated 45%)
adding: solution3$StatsReducer.class(in = 2116) (out= 979)(deflated 53%)
adding: solution3$SalesMapper.class(in = 1990) (out= 817)(deflated 58%)
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -rm -r -f /assign1/task3/out
25/10/01 22:31:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ 
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hadoop jar solution3.jar solution3 \
>   /assign1/task3/in/sales.txt \
>   /assign1/task3/out
25/10/01 22:31:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/10/01 22:31:41 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
25/10/01 22:31:42 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
25/10/01 22:31:42 INFO input.FileInputFormat: Total input paths to process : 1
25/10/01 22:31:42 INFO mapreduce.JobSubmitter: number of splits:1
25/10/01 22:31:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1759319305329_0002
25/10/01 22:31:43 INFO impl.YarnClientImpl: Submitted application application_1759319305329_0002
25/10/01 22:31:43 INFO mapreduce.Job: The url to track the job: http://bigdata-VirtualBox:8088/proxy/application_1759319305329_0002/
25/10/01 22:31:43 INFO mapreduce.Job: Running job: job_1759319305329_0002
25/10/01 22:31:49 INFO mapreduce.Job: Job job_1759319305329_0002 running in uber mode : false
25/10/01 22:31:49 INFO mapreduce.Job:  map 0% reduce 0%
25/10/01 22:31:54 INFO mapreduce.Job:  map 100% reduce 0%
25/10/01 22:32:00 INFO mapreduce.Job:  map 100% reduce 100%
25/10/01 22:32:00 INFO mapreduce.Job: Job job_1759319305329_0002 completed successfully
25/10/01 22:32:00 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=363
		FILE: Number of bytes written=238645
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=399
		HDFS: Number of bytes written=182
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=2889
		Total time spent by all reduces in occupied slots (ms)=2408
		Total time spent by all map tasks (ms)=2889
		Total time spent by all reduce tasks (ms)=2408
		Total vcore-milliseconds taken by all map tasks=2889
		Total vcore-milliseconds taken by all reduce tasks=2408
		Total megabyte-milliseconds taken by all map tasks=2958336
		Total megabyte-milliseconds taken by all reduce tasks=2465792
	Map-Reduce Framework
		Map input records=30
		Map output records=30
		Map output bytes=297
		Map output materialized bytes=363
		Input split bytes=113
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=363
		Reduce input records=30
		Reduce output records=5
		Spilled Records=60
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=162
		CPU time spent (ms)=1310
		Physical memory (bytes) snapshot=441954304
		Virtual memory (bytes) snapshot=3828236288
		Total committed heap usage (bytes)=319815680
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=286
	File Output Format Counters 
		Bytes Written=182
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -ls /assign1/task3/out
25/10/01 22:32:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 2 items
-rw-r--r--   1 bigdata supergroup          0 2025-10-01 22:31 /assign1/task3/out/_SUCCESS
-rw-r--r--   1 bigdata supergroup        182 2025-10-01 22:31 /assign1/task3/out/part-r-00000
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ hdfs dfs -cat /assign1/task3/out/part-r-00000
25/10/01 22:32:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
bolt	MAX=61 MIN=1 AVG=28.29 SUM=198
drill	MAX=14 MIN=1 AVG=5.00 SUM=25
nail	MAX=50 MIN=5 AVG=26.67 SUM=80
screw	MAX=78 MIN=13 AVG=45.38 SUM=363
washer	MAX=56 MIN=3 AVG=21.29 SUM=149
bigdata@bigdata-VirtualBox:~/Desktop/assignment1/task3$ 